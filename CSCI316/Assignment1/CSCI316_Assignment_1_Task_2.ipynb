{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AR8YkD9_JS2C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Build Tree\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        \n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    # By default, we will set the split_criteria to \"Information Gain\" and the max_depth to be 100\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None, split_criteria=\"Information Gain\"):\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.max_depth=max_depth\n",
        "        self.n_features=n_features\n",
        "        self.split_criteria=split_criteria\n",
        "        self.root=None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    # Building subtrees (branches)\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # check the stopping criteria\n",
        "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "\n",
        "        # find the best split\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # create child nodes\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "    # Determine best split bbased on split criteria\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "      best_gain = -1\n",
        "      split_idx, split_threshold = None, None\n",
        "\n",
        "      if self.split_criteria == \"Information Gain\":\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            unique_columns = np.unique(X_column)\n",
        "\n",
        "            for column in unique_columns:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, X_column, column)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = column\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "      if self.split_criteria == \"Gain Ratio\":\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            unique_columns = np.unique(X_column)\n",
        "            \n",
        "            for column in unique_columns:\n",
        "                split_info = 0\n",
        "\n",
        "                # calculate split info\n",
        "                count = Counter(X_column)\n",
        "                prob = count[column] / float(len(X_column))\n",
        "                split_info -= prob * math.log2(prob)\n",
        "\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, X_column, column)\n",
        "\n",
        "                # calculate gain ratio\n",
        "                if split_info != 0:\n",
        "                    gain_ratio = gain / split_info\n",
        "\n",
        "                    if gain_ratio > best_gain:\n",
        "                        best_gain = gain_ratio\n",
        "                        split_idx = feat_idx\n",
        "                        split_threshold = column\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "      if self.split_criteria == \"Gini Index\":\n",
        "        best_gain = 1\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            unique_columns = np.unique(X_column)\n",
        "\n",
        "            for column in unique_columns:\n",
        "                # calculate the gini index\n",
        "                gini = self._gini_index(y, X_column, column)\n",
        "                if gini < best_gain:\n",
        "                    best_gain = gini\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = column\n",
        "    \n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        # parent entropy = base entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # create children\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "        \n",
        "        # calculate the weighted avg. entropy of children\n",
        "        n = len(y)\n",
        "\n",
        "        left_indexes_length, right_indexes_length = len(left_idxs), len(right_idxs)\n",
        "        left_indexes_entropy, right_indexes_entropy = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (left_indexes_length/n) * left_indexes_entropy + (right_indexes_length/n) * right_indexes_entropy\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _gini_index(self, y, X_column, threshold):\n",
        "        value_counts = Counter(X_column)\n",
        "        totalLength = len(X_column)\n",
        "        p_sum = 0\n",
        "\n",
        "        for key in value_counts.keys():\n",
        "            p_sum = p_sum  +  (value_counts[key] / totalLength ) * (value_counts[key] / totalLength ) \n",
        "        gini = 1 - p_sum\n",
        "        return gini\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    # Calculate Entropy\n",
        "    def _entropy(self, data):\n",
        "        # The entropy (uncertainty) of getting a value randomly from the dataset will be higher if the dataset is impure, \n",
        "        # i.e. uneven number of males/females in a dataset\n",
        "\n",
        "        # The higher the impurity in a dataset, the higher the entropy, vice versa.\n",
        "        total = len(data)\n",
        "        count = Counter(data)\n",
        "        ent = 0\n",
        "\n",
        "        for label in count:\n",
        "            prob = count[label] / float(total)\n",
        "            ent -= prob * math.log2(prob)\n",
        "\n",
        "        return ent\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        if y.size == 0:\n",
        "            return 0\n",
        "\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAO0jRCHDTy1"
      },
      "source": [
        "## Now we will add a method called createDataSet which will be used to generate our training and testing data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "canqIVsMcP9q"
      },
      "outputs": [],
      "source": [
        "# Load and Process Data\n",
        "def createDataSet(target=\"\"):\n",
        "    # Import base data in from adult.data\n",
        "    base_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income (class)']\n",
        "    data = pd.read_csv('/adult.data', names=base_labels)\n",
        "    \n",
        "    # Rearrange the columns to have age at the end for classification\n",
        "    labels = base_labels\n",
        "\n",
        "    # Allow the ability to change the target column\n",
        "    if target != \"\":\n",
        "      labels.append(labels.pop(labels.index(target)))\n",
        "      data = data[labels]\n",
        "\n",
        "    # Handle missing values\n",
        "    data = data.fillna(0)  # Fill missing values with 0\n",
        "\n",
        "    # Handle invalid data\n",
        "    data = data.replace(-1, 0)  # Replace invalid values (-1) with 0\n",
        "    \n",
        "    # Randomly shuffle the data\n",
        "    data = data.sample(frac=1)\n",
        "\n",
        "    # Reset the index\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    # Calculate the index to split the data\n",
        "    split_index = int(len(data) * 0.6)\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    train_data = data.iloc[:split_index, :]\n",
        "    train_data_target = train_data[train_data.columns[-1]]\n",
        "    train_data = train_data.iloc[: , :-1]\n",
        "    \n",
        "    test_data = data.iloc[:split_index, :]\n",
        "    test_data_target = test_data[test_data.columns[-1]]\n",
        "    test_data = test_data.iloc[: , :-1]\n",
        "\n",
        "    # Remove last column from labels (for classification)\n",
        "    return train_data.to_numpy(), train_data_target.to_numpy(), test_data.to_numpy(), test_data_target.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVOyaAriCwQV"
      },
      "source": [
        "## We will now start to build and train the decision trees based on the 3 different split criterias namely, Information Gain (dt1), Gain Ratio (dt2) and Gini Index (dt3).\n",
        "\n",
        "## To further improve the training speed while maintaing the model's accuracy, I have added a max_depth feature which is set to a default value of 100, as well as a min_samples_split feature which is set to a default value of 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zpS5iEmMofkM"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "train_data, train_data_target, test_data, test_data_target = createDataSet()\n",
        "\n",
        "dt1 = DecisionTree(max_depth=10)\n",
        "dt2 = DecisionTree(max_depth=10, split_criteria=\"Gain Ratio\")\n",
        "dt3 = DecisionTree(max_depth=10, split_criteria=\"Gini Index\")\n",
        "\n",
        "dt1.fit(copy.deepcopy(train_data), copy.deepcopy(train_data_target))\n",
        "dt2.fit(copy.deepcopy(train_data), copy.deepcopy(train_data_target))\n",
        "dt3.fit(copy.deepcopy(train_data), copy.deepcopy(train_data_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k502kpHDDZ2"
      },
      "source": [
        "## Now we will test the decision trees and store their predictions for analysis. We will use the accuracy method, to determine the accuracy of each decision tree's prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_aPwG5-wYYL",
        "outputId": "5716b579-3f45-4b91-df81-b424f6bec3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8667076167076168\n",
            "0.8263206388206388\n",
            "0.7556818181818182\n"
          ]
        }
      ],
      "source": [
        "predictions1 = dt1.predict(copy.deepcopy(test_data))\n",
        "predictions2 = dt2.predict(copy.deepcopy(test_data))\n",
        "predictions3 = dt3.predict(copy.deepcopy(test_data))\n",
        "\n",
        "def accuracy(y_test, y_pred):\n",
        "    return np.sum(y_test == y_pred) / len(y_test)\n",
        "\n",
        "info_gain_acc = accuracy(test_data_target, predictions1)\n",
        "print(info_gain_acc)\n",
        "\n",
        "gain_ratio_acc = accuracy(test_data_target, predictions2)\n",
        "print(gain_ratio_acc)\n",
        "\n",
        "gini_index_acc = accuracy(test_data_target, predictions3)\n",
        "print(gini_index_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpKUL21P-33f"
      },
      "source": [
        "## Comparing the 3 different split criterias, we can see that **Information Gain has the highest accuracy at 86.67%**, with Gain Ratio at second with an accuracy of 82.63% while Gini Index trails last at 75.57%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7xkmYmAp02E",
        "outputId": "277aed10-5426-44c5-d8c2-4705397971d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8263206388206388\n"
          ]
        }
      ],
      "source": [
        "# Define a function to return the majority vote\n",
        "def ensemble_predict(dt1, dt2, dt3, x):\n",
        "    predictions = [predictions1, predictions2, predictions3]\n",
        "    return [Counter(x).most_common(1)[0][0] for x in zip(*predictions)]\n",
        "\n",
        "# Use the ensemble model to predict on new data\n",
        "ensemble_prediction = ensemble_predict(dt1, dt2, dt3, test_data_target)\n",
        "\n",
        "ensemble_acc = accuracy(test_data_target, ensemble_prediction)\n",
        "print(ensemble_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzBqXdyL-xKj"
      },
      "source": [
        "## As we can see, the ensemble method achieved an accuracy of 82.63%. In comparison with the highest accuracy achieved without the ensemble method, we can see that using Information Gain alone has resulted in a higher accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}